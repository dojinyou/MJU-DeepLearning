{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dEqCsbkWjzBi"
      },
      "outputs": [],
      "source": [
        "# colab 연결용\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor,Lambda\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "U0NPjAoSkEuL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "4rLbKZ9zlL13"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "cn6K5YyM_i4b"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "9jmYBfP8F4z7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "EZ-NB4zIB2-m"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedforward Model\n",
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MLP, self).__init__()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    outputs = self.linear_relu_stack(x)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "xj4qTXsBlOGt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "lr = 0.001\n",
        "epochs = 20\n",
        "model = MLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "SLssKZyBmdRX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  for batch_idx, (X_train, y_train) in enumerate(train_dataloader):\n",
        "    # Train\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "    pred = model(X_train)\n",
        "    loss = criterion(pred, y_train)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if batch_idx % 100 == 0:\n",
        "      loss, current = loss.item(), batch_idx * len(X_train)\n",
        "      print(f\"loss : {loss::>7f} [{current:>5d}/{len(train_dataloader.dataset):>5d}]\")\n",
        "      \n",
        "  test_loss, correct = 0,0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for X_test,y_test in test_dataloader:\n",
        "      X_test = X_test.to(device)\n",
        "      y_test = y_test.to(device)\n",
        "      pred = model(X_test)\n",
        "      test_loss += criterion(pred,y_test).item()\n",
        "      correct += (pred.argmax(1) == y_test).type(torch.float).sum().item()\n",
        "    test_loss /= len(test_dataloader)\n",
        "    correct /= len(test_dataloader.dataset)\n",
        "    print(f\"Test Error:\\n Accurancy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8}\\n\")\n"
      ],
      "metadata": {
        "id": "xT4zn8UMCh3y",
        "outputId": "671456e3-9bab-42ef-fbc8-8ed13375d759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss : 2.337666 [    0/60000]\n",
            "loss : 1.417882 [ 6400/60000]\n",
            "loss : 1.152488 [12800/60000]\n",
            "loss : 1.092979 [19200/60000]\n",
            "loss : 1.015640 [25600/60000]\n",
            "loss : 0.906772 [32000/60000]\n",
            "loss : 0.702210 [38400/60000]\n",
            "loss : 0.836317 [44800/60000]\n",
            "loss : 0.862248 [51200/60000]\n",
            "loss : 0.717844 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 79.3%, Avg loss: 0.6517241878041835\n",
            "\n",
            "loss : 0.639922 [    0/60000]\n",
            "loss : 0.633773 [ 6400/60000]\n",
            "loss : 0.763333 [12800/60000]\n",
            "loss : 0.729423 [19200/60000]\n",
            "loss : 0.751118 [25600/60000]\n",
            "loss : 0.650236 [32000/60000]\n",
            "loss : 0.496598 [38400/60000]\n",
            "loss : 0.574316 [44800/60000]\n",
            "loss : 0.685227 [51200/60000]\n",
            "loss : 0.531638 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 82.4%, Avg loss: 0.5287218895166922\n",
            "\n",
            "loss : 0.521707 [    0/60000]\n",
            "loss : 0.670882 [ 6400/60000]\n",
            "loss : 0.596616 [12800/60000]\n",
            "loss : 0.536481 [19200/60000]\n",
            "loss : 0.413756 [25600/60000]\n",
            "loss : 0.536480 [32000/60000]\n",
            "loss : 0.528198 [38400/60000]\n",
            "loss : 0.477030 [44800/60000]\n",
            "loss : 0.379460 [51200/60000]\n",
            "loss : 0.499761 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 84.1%, Avg loss: 0.47171793263286416\n",
            "\n",
            "loss : 0.396448 [    0/60000]\n",
            "loss : 0.391465 [ 6400/60000]\n",
            "loss : 0.551724 [12800/60000]\n",
            "loss : 0.471719 [19200/60000]\n",
            "loss : 0.414191 [25600/60000]\n",
            "loss : 0.445812 [32000/60000]\n",
            "loss : 0.471847 [38400/60000]\n",
            "loss : 0.340447 [44800/60000]\n",
            "loss : 0.459917 [51200/60000]\n",
            "loss : 0.478768 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 85.1%, Avg loss: 0.4370981393846622\n",
            "\n",
            "loss : 0.429125 [    0/60000]\n",
            "loss : 0.554344 [ 6400/60000]\n",
            "loss : 0.397651 [12800/60000]\n",
            "loss : 0.369839 [19200/60000]\n",
            "loss : 0.475401 [25600/60000]\n",
            "loss : 0.318559 [32000/60000]\n",
            "loss : 0.670339 [38400/60000]\n",
            "loss : 0.545720 [44800/60000]\n",
            "loss : 0.521123 [51200/60000]\n",
            "loss : 0.454725 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 85.8%, Avg loss: 0.4113046824773237\n",
            "\n",
            "loss : 0.385673 [    0/60000]\n",
            "loss : 0.335298 [ 6400/60000]\n",
            "loss : 0.476389 [12800/60000]\n",
            "loss : 0.293383 [19200/60000]\n",
            "loss : 0.371296 [25600/60000]\n",
            "loss : 0.296926 [32000/60000]\n",
            "loss : 0.302259 [38400/60000]\n",
            "loss : 0.391129 [44800/60000]\n",
            "loss : 0.254629 [51200/60000]\n",
            "loss : 0.330452 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 86.4%, Avg loss: 0.39220483466061445\n",
            "\n",
            "loss : 0.388461 [    0/60000]\n",
            "loss : 0.495182 [ 6400/60000]\n",
            "loss : 0.418080 [12800/60000]\n",
            "loss : 0.290350 [19200/60000]\n",
            "loss : 0.448532 [25600/60000]\n",
            "loss : 0.457429 [32000/60000]\n",
            "loss : 0.438436 [38400/60000]\n",
            "loss : 0.305453 [44800/60000]\n",
            "loss : 0.280284 [51200/60000]\n",
            "loss : 0.594771 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 87.0%, Avg loss: 0.3754428357743759\n",
            "\n",
            "loss : 0.490229 [    0/60000]\n",
            "loss : 0.464848 [ 6400/60000]\n",
            "loss : 0.498944 [12800/60000]\n",
            "loss : 0.319634 [19200/60000]\n",
            "loss : 0.420541 [25600/60000]\n",
            "loss : 0.364282 [32000/60000]\n",
            "loss : 0.454590 [38400/60000]\n",
            "loss : 0.461186 [44800/60000]\n",
            "loss : 0.321191 [51200/60000]\n",
            "loss : 0.308354 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 87.4%, Avg loss: 0.36356778325302513\n",
            "\n",
            "loss : 0.334514 [    0/60000]\n",
            "loss : 0.425543 [ 6400/60000]\n",
            "loss : 0.314032 [12800/60000]\n",
            "loss : 0.371546 [19200/60000]\n",
            "loss : 0.375655 [25600/60000]\n",
            "loss : 0.309113 [32000/60000]\n",
            "loss : 0.322443 [38400/60000]\n",
            "loss : 0.354646 [44800/60000]\n",
            "loss : 0.265219 [51200/60000]\n",
            "loss : 0.233013 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 87.7%, Avg loss: 0.3529572795544352\n",
            "\n",
            "loss : 0.539153 [    0/60000]\n",
            "loss : 0.588810 [ 6400/60000]\n",
            "loss : 0.372184 [12800/60000]\n",
            "loss : 0.472643 [19200/60000]\n",
            "loss : 0.313658 [25600/60000]\n",
            "loss : 0.238461 [32000/60000]\n",
            "loss : 0.311945 [38400/60000]\n",
            "loss : 0.297183 [44800/60000]\n",
            "loss : 0.376353 [51200/60000]\n",
            "loss : 0.436858 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 87.9%, Avg loss: 0.34345089262927264\n",
            "\n",
            "loss : 0.364160 [    0/60000]\n",
            "loss : 0.274744 [ 6400/60000]\n",
            "loss : 0.346387 [12800/60000]\n",
            "loss : 0.236153 [19200/60000]\n",
            "loss : 0.298479 [25600/60000]\n",
            "loss : 0.312524 [32000/60000]\n",
            "loss : 0.281368 [38400/60000]\n",
            "loss : 0.293426 [44800/60000]\n",
            "loss : 0.420819 [51200/60000]\n",
            "loss : 0.205483 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 88.3%, Avg loss: 0.3351065926015504\n",
            "\n",
            "loss : 0.347350 [    0/60000]\n",
            "loss : 0.221180 [ 6400/60000]\n",
            "loss : 0.308248 [12800/60000]\n",
            "loss : 0.280244 [19200/60000]\n",
            "loss : 0.427475 [25600/60000]\n",
            "loss : 0.437425 [32000/60000]\n",
            "loss : 0.501155 [38400/60000]\n",
            "loss : 0.456974 [44800/60000]\n",
            "loss : 0.301120 [51200/60000]\n",
            "loss : 0.401319 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 88.7%, Avg loss: 0.3246258430039959\n",
            "\n",
            "loss : 0.301381 [    0/60000]\n",
            "loss : 0.291323 [ 6400/60000]\n",
            "loss : 0.297692 [12800/60000]\n",
            "loss : 0.271956 [19200/60000]\n",
            "loss : 0.463944 [25600/60000]\n",
            "loss : 0.361273 [32000/60000]\n",
            "loss : 0.337155 [38400/60000]\n",
            "loss : 0.405940 [44800/60000]\n",
            "loss : 0.268094 [51200/60000]\n",
            "loss : 0.406614 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 88.8%, Avg loss: 0.3209459112206502\n",
            "\n",
            "loss : 0.272865 [    0/60000]\n",
            "loss : 0.274831 [ 6400/60000]\n",
            "loss : 0.377538 [12800/60000]\n",
            "loss : 0.255647 [19200/60000]\n",
            "loss : 0.384831 [25600/60000]\n",
            "loss : 0.395091 [32000/60000]\n",
            "loss : 0.349335 [38400/60000]\n",
            "loss : 0.359220 [44800/60000]\n",
            "loss : 0.283884 [51200/60000]\n",
            "loss : 0.296343 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 89.2%, Avg loss: 0.3112347152854588\n",
            "\n",
            "loss : 0.305144 [    0/60000]\n",
            "loss : 0.312242 [ 6400/60000]\n",
            "loss : 0.326741 [12800/60000]\n",
            "loss : 0.400073 [19200/60000]\n",
            "loss : 0.308272 [25600/60000]\n",
            "loss : 0.480043 [32000/60000]\n",
            "loss : 0.410588 [38400/60000]\n",
            "loss : 0.279638 [44800/60000]\n",
            "loss : 0.343314 [51200/60000]\n",
            "loss : 0.314547 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 89.4%, Avg loss: 0.30516042072635724\n",
            "\n",
            "loss : 0.370573 [    0/60000]\n",
            "loss : 0.255435 [ 6400/60000]\n",
            "loss : 0.274782 [12800/60000]\n",
            "loss : 0.333905 [19200/60000]\n",
            "loss : 0.270055 [25600/60000]\n",
            "loss : 0.385390 [32000/60000]\n",
            "loss : 0.343339 [38400/60000]\n",
            "loss : 0.399820 [44800/60000]\n",
            "loss : 0.217356 [51200/60000]\n",
            "loss : 0.285421 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 89.5%, Avg loss: 0.29952260547641246\n",
            "\n",
            "loss : 0.352820 [    0/60000]\n",
            "loss : 0.178292 [ 6400/60000]\n",
            "loss : 0.187082 [12800/60000]\n",
            "loss : 0.334511 [19200/60000]\n",
            "loss : 0.338568 [25600/60000]\n",
            "loss : 0.231171 [32000/60000]\n",
            "loss : 0.500625 [38400/60000]\n",
            "loss : 0.301177 [44800/60000]\n",
            "loss : 0.240902 [51200/60000]\n",
            "loss : 0.317824 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 89.7%, Avg loss: 0.2951148693829076\n",
            "\n",
            "loss : 0.299519 [    0/60000]\n",
            "loss : 0.326843 [ 6400/60000]\n",
            "loss : 0.400125 [12800/60000]\n",
            "loss : 0.316859 [19200/60000]\n",
            "loss : 0.284070 [25600/60000]\n",
            "loss : 0.314298 [32000/60000]\n",
            "loss : 0.413195 [38400/60000]\n",
            "loss : 0.437126 [44800/60000]\n",
            "loss : 0.281010 [51200/60000]\n",
            "loss : 0.361991 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 89.9%, Avg loss: 0.287877541869434\n",
            "\n",
            "loss : 0.272972 [    0/60000]\n",
            "loss : 0.337534 [ 6400/60000]\n",
            "loss : 0.367939 [12800/60000]\n",
            "loss : 0.312001 [19200/60000]\n",
            "loss : 0.247661 [25600/60000]\n",
            "loss : 0.330566 [32000/60000]\n",
            "loss : 0.266997 [38400/60000]\n",
            "loss : 0.349693 [44800/60000]\n",
            "loss : 0.329228 [51200/60000]\n",
            "loss : 0.341518 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 90.2%, Avg loss: 0.2827086371145269\n",
            "\n",
            "loss : 0.377617 [    0/60000]\n",
            "loss : 0.335491 [ 6400/60000]\n",
            "loss : 0.404280 [12800/60000]\n",
            "loss : 0.240591 [19200/60000]\n",
            "loss : 0.298566 [25600/60000]\n",
            "loss : 0.478746 [32000/60000]\n",
            "loss : 0.358323 [38400/60000]\n",
            "loss : 0.303493 [44800/60000]\n",
            "loss : 0.214355 [51200/60000]\n",
            "loss : 0.285413 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 90.3%, Avg loss: 0.27798794784239617\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedforward Model\n",
        "class NoBatchNormMLP(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NoBatchNormMLP, self).__init__()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    outputs = self.linear_relu_stack(x)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "pJz1sEiR7pKz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_batch_norm_model = NoBatchNormMLP().to(device)\n",
        "optimizer = torch.optim.SGD(no_batch_norm_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "rtewZnBpGp1P"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  for batch_idx, (X_train, y_train) in enumerate(train_dataloader):\n",
        "    # Train\n",
        "    no_batch_norm_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "    pred = no_batch_norm_model(X_train)\n",
        "    loss = criterion(pred, y_train)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if batch_idx % 100 == 0:\n",
        "      loss, current = loss.item(), batch_idx * len(X_train)\n",
        "      print(f\"loss : {loss::>7f} [{current:>5d}/{len(train_dataloader.dataset):>5d}]\")\n",
        "  \n",
        "  test_loss, correct = 0,0\n",
        "  no_batch_norm_model.eval()\n",
        "  with torch.no_grad():\n",
        "    for X_test,y_test in test_dataloader:\n",
        "      X_test = X_test.to(device)\n",
        "      y_test = y_test.to(device)\n",
        "      pred = no_batch_norm_model(X_test)\n",
        "      test_loss += criterion(pred,y_test).item()\n",
        "      correct += (pred.argmax(1) == y_test).type(torch.float).sum().item()\n",
        "    test_loss /= len(test_dataloader)\n",
        "    correct /= len(test_dataloader.dataset)\n",
        "    print(f\"Test Error:\\n Accurancy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8}\\n\")"
      ],
      "metadata": {
        "id": "A41p2sWKG_Y-",
        "outputId": "21cca881-1514-4f61-cac8-4c20a5285095",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss : 2.312639 [    0/60000]\n",
            "loss : 2.292538 [ 6400/60000]\n",
            "loss : 2.278856 [12800/60000]\n",
            "loss : 2.270298 [19200/60000]\n",
            "loss : 2.255163 [25600/60000]\n",
            "loss : 2.232386 [32000/60000]\n",
            "loss : 2.215727 [38400/60000]\n",
            "loss : 2.194212 [44800/60000]\n",
            "loss : 2.163147 [51200/60000]\n",
            "loss : 2.160558 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 44.5%, Avg loss: 2.146650001946797\n",
            "\n",
            "loss : 2.142972 [    0/60000]\n",
            "loss : 2.134417 [ 6400/60000]\n",
            "loss : 2.074998 [12800/60000]\n",
            "loss : 2.055355 [19200/60000]\n",
            "loss : 2.045841 [25600/60000]\n",
            "loss : 1.962455 [32000/60000]\n",
            "loss : 1.972443 [38400/60000]\n",
            "loss : 1.980720 [44800/60000]\n",
            "loss : 1.912199 [51200/60000]\n",
            "loss : 1.849924 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 52.0%, Avg loss: 1.862774512788126\n",
            "\n",
            "loss : 1.895276 [    0/60000]\n",
            "loss : 1.783568 [ 6400/60000]\n",
            "loss : 1.769848 [12800/60000]\n",
            "loss : 1.712306 [19200/60000]\n",
            "loss : 1.743461 [25600/60000]\n",
            "loss : 1.599569 [32000/60000]\n",
            "loss : 1.670750 [38400/60000]\n",
            "loss : 1.610771 [44800/60000]\n",
            "loss : 1.528476 [51200/60000]\n",
            "loss : 1.619445 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 60.3%, Avg loss: 1.5080574842404202\n",
            "\n",
            "loss : 1.512955 [    0/60000]\n",
            "loss : 1.568623 [ 6400/60000]\n",
            "loss : 1.384417 [12800/60000]\n",
            "loss : 1.374616 [19200/60000]\n",
            "loss : 1.330373 [25600/60000]\n",
            "loss : 1.329285 [32000/60000]\n",
            "loss : 1.405046 [38400/60000]\n",
            "loss : 1.268552 [44800/60000]\n",
            "loss : 1.357344 [51200/60000]\n",
            "loss : 1.232563 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 63.6%, Avg loss: 1.2581354537244036\n",
            "\n",
            "loss : 1.379406 [    0/60000]\n",
            "loss : 1.226851 [ 6400/60000]\n",
            "loss : 1.200010 [12800/60000]\n",
            "loss : 1.260107 [19200/60000]\n",
            "loss : 1.343455 [25600/60000]\n",
            "loss : 1.165952 [32000/60000]\n",
            "loss : 1.168206 [38400/60000]\n",
            "loss : 1.158379 [44800/60000]\n",
            "loss : 1.139756 [51200/60000]\n",
            "loss : 1.203435 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 65.5%, Avg loss: 1.0977053606688087\n",
            "\n",
            "loss : 1.025864 [    0/60000]\n",
            "loss : 1.147345 [ 6400/60000]\n",
            "loss : 1.026182 [12800/60000]\n",
            "loss : 0.973407 [19200/60000]\n",
            "loss : 0.936352 [25600/60000]\n",
            "loss : 1.031978 [32000/60000]\n",
            "loss : 1.015776 [38400/60000]\n",
            "loss : 1.000524 [44800/60000]\n",
            "loss : 0.953166 [51200/60000]\n",
            "loss : 0.944754 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 66.5%, Avg loss: 0.9893719562208221\n",
            "\n",
            "loss : 1.093522 [    0/60000]\n",
            "loss : 1.007484 [ 6400/60000]\n",
            "loss : 0.983634 [12800/60000]\n",
            "loss : 0.969469 [19200/60000]\n",
            "loss : 0.998366 [25600/60000]\n",
            "loss : 0.954588 [32000/60000]\n",
            "loss : 1.111955 [38400/60000]\n",
            "loss : 1.050405 [44800/60000]\n",
            "loss : 0.903750 [51200/60000]\n",
            "loss : 0.781021 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 68.5%, Avg loss: 0.9117018668763419\n",
            "\n",
            "loss : 0.822924 [    0/60000]\n",
            "loss : 0.886166 [ 6400/60000]\n",
            "loss : 0.940571 [12800/60000]\n",
            "loss : 0.834025 [19200/60000]\n",
            "loss : 1.030764 [25600/60000]\n",
            "loss : 0.781166 [32000/60000]\n",
            "loss : 0.930934 [38400/60000]\n",
            "loss : 1.170377 [44800/60000]\n",
            "loss : 0.712710 [51200/60000]\n",
            "loss : 0.818902 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 70.1%, Avg loss: 0.8548237720150937\n",
            "\n",
            "loss : 0.794071 [    0/60000]\n",
            "loss : 0.967652 [ 6400/60000]\n",
            "loss : 0.832491 [12800/60000]\n",
            "loss : 0.745268 [19200/60000]\n",
            "loss : 0.821351 [25600/60000]\n",
            "loss : 0.802060 [32000/60000]\n",
            "loss : 0.825789 [38400/60000]\n",
            "loss : 0.833863 [44800/60000]\n",
            "loss : 0.751078 [51200/60000]\n",
            "loss : 0.771789 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 71.6%, Avg loss: 0.8114621181096604\n",
            "\n",
            "loss : 0.927985 [    0/60000]\n",
            "loss : 0.770025 [ 6400/60000]\n",
            "loss : 0.887300 [12800/60000]\n",
            "loss : 0.743346 [19200/60000]\n",
            "loss : 0.776386 [25600/60000]\n",
            "loss : 0.979101 [32000/60000]\n",
            "loss : 0.817776 [38400/60000]\n",
            "loss : 0.900219 [44800/60000]\n",
            "loss : 0.894302 [51200/60000]\n",
            "loss : 0.819882 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 71.9%, Avg loss: 0.7774644362519799\n",
            "\n",
            "loss : 0.754132 [    0/60000]\n",
            "loss : 0.649049 [ 6400/60000]\n",
            "loss : 0.657982 [12800/60000]\n",
            "loss : 0.949558 [19200/60000]\n",
            "loss : 0.892832 [25600/60000]\n",
            "loss : 0.691819 [32000/60000]\n",
            "loss : 0.665166 [38400/60000]\n",
            "loss : 0.741482 [44800/60000]\n",
            "loss : 0.808470 [51200/60000]\n",
            "loss : 0.871111 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 73.2%, Avg loss: 0.7488529602411205\n",
            "\n",
            "loss : 0.570948 [    0/60000]\n",
            "loss : 0.673255 [ 6400/60000]\n",
            "loss : 0.723095 [12800/60000]\n",
            "loss : 0.640141 [19200/60000]\n",
            "loss : 0.746378 [25600/60000]\n",
            "loss : 0.771459 [32000/60000]\n",
            "loss : 0.803147 [38400/60000]\n",
            "loss : 0.677089 [44800/60000]\n",
            "loss : 0.727939 [51200/60000]\n",
            "loss : 0.706722 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 74.6%, Avg loss: 0.7249209139901184\n",
            "\n",
            "loss : 0.615427 [    0/60000]\n",
            "loss : 0.741111 [ 6400/60000]\n",
            "loss : 0.647935 [12800/60000]\n",
            "loss : 0.776215 [19200/60000]\n",
            "loss : 0.768369 [25600/60000]\n",
            "loss : 0.765706 [32000/60000]\n",
            "loss : 0.830513 [38400/60000]\n",
            "loss : 0.707303 [44800/60000]\n",
            "loss : 0.609623 [51200/60000]\n",
            "loss : 0.726468 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 75.3%, Avg loss: 0.7045076460535846\n",
            "\n",
            "loss : 0.793709 [    0/60000]\n",
            "loss : 0.663275 [ 6400/60000]\n",
            "loss : 0.595211 [12800/60000]\n",
            "loss : 0.604596 [19200/60000]\n",
            "loss : 0.592615 [25600/60000]\n",
            "loss : 0.654451 [32000/60000]\n",
            "loss : 0.925485 [38400/60000]\n",
            "loss : 0.756472 [44800/60000]\n",
            "loss : 0.596436 [51200/60000]\n",
            "loss : 0.664158 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 76.1%, Avg loss: 0.6851458633695838\n",
            "\n",
            "loss : 0.466590 [    0/60000]\n",
            "loss : 0.916179 [ 6400/60000]\n",
            "loss : 0.600496 [12800/60000]\n",
            "loss : 0.664554 [19200/60000]\n",
            "loss : 0.801507 [25600/60000]\n",
            "loss : 0.869188 [32000/60000]\n",
            "loss : 0.611364 [38400/60000]\n",
            "loss : 0.665068 [44800/60000]\n",
            "loss : 0.712984 [51200/60000]\n",
            "loss : 0.887592 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 76.9%, Avg loss: 0.6679942746406425\n",
            "\n",
            "loss : 0.683082 [    0/60000]\n",
            "loss : 0.735297 [ 6400/60000]\n",
            "loss : 0.742590 [12800/60000]\n",
            "loss : 0.620045 [19200/60000]\n",
            "loss : 0.420303 [25600/60000]\n",
            "loss : 0.659369 [32000/60000]\n",
            "loss : 0.681900 [38400/60000]\n",
            "loss : 0.601107 [44800/60000]\n",
            "loss : 0.632199 [51200/60000]\n",
            "loss : 0.571282 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 77.3%, Avg loss: 0.6527266402297945\n",
            "\n",
            "loss : 0.652950 [    0/60000]\n",
            "loss : 0.842674 [ 6400/60000]\n",
            "loss : 0.734911 [12800/60000]\n",
            "loss : 0.711733 [19200/60000]\n",
            "loss : 0.534532 [25600/60000]\n",
            "loss : 0.619482 [32000/60000]\n",
            "loss : 0.545164 [38400/60000]\n",
            "loss : 0.655885 [44800/60000]\n",
            "loss : 0.609453 [51200/60000]\n",
            "loss : 0.506105 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 78.1%, Avg loss: 0.6373188757121182\n",
            "\n",
            "loss : 0.511542 [    0/60000]\n",
            "loss : 0.648984 [ 6400/60000]\n",
            "loss : 0.543364 [12800/60000]\n",
            "loss : 0.691078 [19200/60000]\n",
            "loss : 0.654039 [25600/60000]\n",
            "loss : 0.614646 [32000/60000]\n",
            "loss : 0.566277 [38400/60000]\n",
            "loss : 0.606798 [44800/60000]\n",
            "loss : 0.546083 [51200/60000]\n",
            "loss : 0.704266 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 78.8%, Avg loss: 0.6242317642484393\n",
            "\n",
            "loss : 0.658797 [    0/60000]\n",
            "loss : 0.624553 [ 6400/60000]\n",
            "loss : 0.550870 [12800/60000]\n",
            "loss : 0.596010 [19200/60000]\n",
            "loss : 0.517672 [25600/60000]\n",
            "loss : 0.600937 [32000/60000]\n",
            "loss : 0.579562 [38400/60000]\n",
            "loss : 0.647484 [44800/60000]\n",
            "loss : 0.570495 [51200/60000]\n",
            "loss : 0.657966 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 79.1%, Avg loss: 0.6116461878074512\n",
            "\n",
            "loss : 0.734983 [    0/60000]\n",
            "loss : 0.641044 [ 6400/60000]\n",
            "loss : 0.548826 [12800/60000]\n",
            "loss : 0.473251 [19200/60000]\n",
            "loss : 0.732522 [25600/60000]\n",
            "loss : 0.512498 [32000/60000]\n",
            "loss : 0.567910 [38400/60000]\n",
            "loss : 0.584464 [44800/60000]\n",
            "loss : 0.505405 [51200/60000]\n",
            "loss : 0.559445 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 79.7%, Avg loss: 0.600253726151198\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "KXwgoaNYJSnE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = torch.zeros(1)\n",
        "std = torch.zeros(1)\n",
        "print('==> Computing mean and std..')\n",
        "for inputs, _labels in train_dataloader:\n",
        "  for i in range(1):\n",
        "    mean[i] += inputs[:,i,:,:].mean()\n",
        "    std[i] += inputs[:,i,:,:].std()\n",
        "  mean.div_(len(train_dataloader))\n",
        "  std.div_(len(train_dataloader))\n",
        "print(mean, std)\n"
      ],
      "metadata": {
        "id": "N_tG8ZHnNPjb",
        "outputId": "6dc5a9ee-d7e7-4c0d-ed0e-2107ac888f85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Computing mean and std..\n",
            "tensor([0.0003]) tensor([0.0004])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.0003), (0.0004))\n",
        "])"
      ],
      "metadata": {
        "id": "Zxiw4JVgHIu3"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_training_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=norm_transform,\n",
        ")"
      ],
      "metadata": {
        "id": "pXkHagCHJR6U"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_test_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=norm_transform\n",
        ")"
      ],
      "metadata": {
        "id": "JGMxaZDDJmg1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_train_dataloader = DataLoader(norm_training_data, batch_size=64, shuffle=True)\n",
        "norm_test_dataloader = DataLoader(norm_test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "gUGnOQj9JtgJ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_norm = MLP().to(device)\n",
        "optimizer = torch.optim.SGD(model_for_norm.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "2xKh-fe_JzTK"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  for batch_idx, (X_train, y_train) in enumerate(norm_train_dataloader):\n",
        "    # Train\n",
        "    model_for_norm.train()\n",
        "    optimizer.zero_grad()\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "    pred = model_for_norm(X_train)\n",
        "    loss = criterion(pred, y_train)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if batch_idx % 100 == 0:\n",
        "      loss, current = loss.item(), batch_idx * len(X_train)\n",
        "      print(f\"loss : {loss::>7f} [{current:>5d}/{len(norm_train_dataloader.dataset):>5d}]\")\n",
        "  \n",
        "  test_loss, correct = 0,0\n",
        "  model_for_norm.eval()\n",
        "  with torch.no_grad():\n",
        "    for X_test,y_test in norm_test_dataloader:\n",
        "      X_test = X_test.to(device)\n",
        "      y_test = y_test.to(device)\n",
        "      pred = model_for_norm(X_test)\n",
        "      test_loss += criterion(pred,y_test).item()\n",
        "      correct += (pred.argmax(1) == y_test).type(torch.float).sum().item()\n",
        "    test_loss /= len(norm_test_dataloader)\n",
        "    correct /= len(norm_test_dataloader.dataset)\n",
        "    print(f\"Test Error:\\n Accurancy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8}\\n\")\n"
      ],
      "metadata": {
        "id": "MjopAAizJ-HT",
        "outputId": "ac0f3069-c3b1-4ca9-c389-ff7f4f662db4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss : 2.372258 [    0/60000]\n",
            "loss : 1.310358 [ 6400/60000]\n",
            "loss : 1.181325 [12800/60000]\n",
            "loss : 0.871238 [19200/60000]\n",
            "loss : 0.725590 [25600/60000]\n",
            "loss : 0.617003 [32000/60000]\n",
            "loss : 0.776429 [38400/60000]\n",
            "loss : 0.780041 [44800/60000]\n",
            "loss : 0.725992 [51200/60000]\n",
            "loss : 0.681127 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 78.4%, Avg loss: 0.6659049306325852\n",
            "\n",
            "loss : 0.571479 [    0/60000]\n",
            "loss : 0.574710 [ 6400/60000]\n",
            "loss : 0.684689 [12800/60000]\n",
            "loss : 0.712088 [19200/60000]\n",
            "loss : 0.667750 [25600/60000]\n",
            "loss : 0.672077 [32000/60000]\n",
            "loss : 0.623117 [38400/60000]\n",
            "loss : 0.551999 [44800/60000]\n",
            "loss : 0.553998 [51200/60000]\n",
            "loss : 0.541286 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 81.6%, Avg loss: 0.5514632626703591\n",
            "\n",
            "loss : 0.531536 [    0/60000]\n",
            "loss : 0.515339 [ 6400/60000]\n",
            "loss : 0.571115 [12800/60000]\n",
            "loss : 0.656290 [19200/60000]\n",
            "loss : 0.367349 [25600/60000]\n",
            "loss : 0.490077 [32000/60000]\n",
            "loss : 0.469596 [38400/60000]\n",
            "loss : 0.399151 [44800/60000]\n",
            "loss : 0.647262 [51200/60000]\n",
            "loss : 0.435325 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 82.7%, Avg loss: 0.5019901882690988\n",
            "\n",
            "loss : 0.599644 [    0/60000]\n",
            "loss : 0.466754 [ 6400/60000]\n",
            "loss : 0.680699 [12800/60000]\n",
            "loss : 0.627442 [19200/60000]\n",
            "loss : 0.678158 [25600/60000]\n",
            "loss : 0.373717 [32000/60000]\n",
            "loss : 0.552015 [38400/60000]\n",
            "loss : 0.549098 [44800/60000]\n",
            "loss : 0.466359 [51200/60000]\n",
            "loss : 0.449316 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 83.6%, Avg loss: 0.46959245945238004\n",
            "\n",
            "loss : 0.419040 [    0/60000]\n",
            "loss : 0.675476 [ 6400/60000]\n",
            "loss : 0.448214 [12800/60000]\n",
            "loss : 0.399646 [19200/60000]\n",
            "loss : 0.412649 [25600/60000]\n",
            "loss : 0.591598 [32000/60000]\n",
            "loss : 0.404240 [38400/60000]\n",
            "loss : 0.457634 [44800/60000]\n",
            "loss : 0.711494 [51200/60000]\n",
            "loss : 0.438539 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 84.1%, Avg loss: 0.4484136782254383\n",
            "\n",
            "loss : 0.532736 [    0/60000]\n",
            "loss : 0.361037 [ 6400/60000]\n",
            "loss : 0.477943 [12800/60000]\n",
            "loss : 0.350343 [19200/60000]\n",
            "loss : 0.450408 [25600/60000]\n",
            "loss : 0.421369 [32000/60000]\n",
            "loss : 0.488368 [38400/60000]\n",
            "loss : 0.402531 [44800/60000]\n",
            "loss : 0.314838 [51200/60000]\n",
            "loss : 0.490137 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 84.6%, Avg loss: 0.4324174287023058\n",
            "\n",
            "loss : 0.520467 [    0/60000]\n",
            "loss : 0.326183 [ 6400/60000]\n",
            "loss : 0.300575 [12800/60000]\n",
            "loss : 0.465885 [19200/60000]\n",
            "loss : 0.343732 [25600/60000]\n",
            "loss : 0.425651 [32000/60000]\n",
            "loss : 0.363501 [38400/60000]\n",
            "loss : 0.398339 [44800/60000]\n",
            "loss : 0.402088 [51200/60000]\n",
            "loss : 0.347305 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 84.9%, Avg loss: 0.420238460135308\n",
            "\n",
            "loss : 0.437971 [    0/60000]\n",
            "loss : 0.371510 [ 6400/60000]\n",
            "loss : 0.310887 [12800/60000]\n",
            "loss : 0.525718 [19200/60000]\n",
            "loss : 0.399571 [25600/60000]\n",
            "loss : 0.399707 [32000/60000]\n",
            "loss : 0.434408 [38400/60000]\n",
            "loss : 0.505926 [44800/60000]\n",
            "loss : 0.299215 [51200/60000]\n",
            "loss : 0.350693 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 85.2%, Avg loss: 0.4108276115671085\n",
            "\n",
            "loss : 0.665124 [    0/60000]\n",
            "loss : 0.286607 [ 6400/60000]\n",
            "loss : 0.349532 [12800/60000]\n",
            "loss : 0.341592 [19200/60000]\n",
            "loss : 0.336746 [25600/60000]\n",
            "loss : 0.388800 [32000/60000]\n",
            "loss : 0.418541 [38400/60000]\n",
            "loss : 0.410113 [44800/60000]\n",
            "loss : 0.323392 [51200/60000]\n",
            "loss : 0.378096 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 85.6%, Avg loss: 0.402534083196312\n",
            "\n",
            "loss : 0.363766 [    0/60000]\n",
            "loss : 0.366895 [ 6400/60000]\n",
            "loss : 0.374739 [12800/60000]\n",
            "loss : 0.305437 [19200/60000]\n",
            "loss : 0.438456 [25600/60000]\n",
            "loss : 0.494958 [32000/60000]\n",
            "loss : 0.358871 [38400/60000]\n",
            "loss : 0.358356 [44800/60000]\n",
            "loss : 0.321927 [51200/60000]\n",
            "loss : 0.418169 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 86.0%, Avg loss: 0.39448285026914753\n",
            "\n",
            "loss : 0.359443 [    0/60000]\n",
            "loss : 0.459249 [ 6400/60000]\n",
            "loss : 0.398418 [12800/60000]\n",
            "loss : 0.419758 [19200/60000]\n",
            "loss : 0.288317 [25600/60000]\n",
            "loss : 0.508388 [32000/60000]\n",
            "loss : 0.358148 [38400/60000]\n",
            "loss : 0.550998 [44800/60000]\n",
            "loss : 0.403419 [51200/60000]\n",
            "loss : 0.252215 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 86.1%, Avg loss: 0.3872800561462998\n",
            "\n",
            "loss : 0.311186 [    0/60000]\n",
            "loss : 0.688021 [ 6400/60000]\n",
            "loss : 0.254625 [12800/60000]\n",
            "loss : 0.397920 [19200/60000]\n",
            "loss : 0.469537 [25600/60000]\n",
            "loss : 0.286771 [32000/60000]\n",
            "loss : 0.369826 [38400/60000]\n",
            "loss : 0.314499 [44800/60000]\n",
            "loss : 0.267687 [51200/60000]\n",
            "loss : 0.359157 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 86.2%, Avg loss: 0.38204778949166557\n",
            "\n",
            "loss : 0.386931 [    0/60000]\n",
            "loss : 0.221905 [ 6400/60000]\n",
            "loss : 0.256369 [12800/60000]\n",
            "loss : 0.280323 [19200/60000]\n",
            "loss : 0.266866 [25600/60000]\n",
            "loss : 0.367150 [32000/60000]\n",
            "loss : 0.223959 [38400/60000]\n",
            "loss : 0.345360 [44800/60000]\n",
            "loss : 0.330678 [51200/60000]\n",
            "loss : 0.558206 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 86.5%, Avg loss: 0.37633116106698467\n",
            "\n",
            "loss : 0.344258 [    0/60000]\n",
            "loss : 0.346397 [ 6400/60000]\n",
            "loss : 0.356603 [12800/60000]\n",
            "loss : 0.315799 [19200/60000]\n",
            "loss : 0.401326 [25600/60000]\n",
            "loss : 0.262371 [32000/60000]\n",
            "loss : 0.354797 [38400/60000]\n",
            "loss : 0.400554 [44800/60000]\n",
            "loss : 0.246714 [51200/60000]\n",
            "loss : 0.386907 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 86.6%, Avg loss: 0.37251177714888456\n",
            "\n",
            "loss : 0.276887 [    0/60000]\n",
            "loss : 0.432006 [ 6400/60000]\n",
            "loss : 0.432118 [12800/60000]\n",
            "loss : 0.223780 [19200/60000]\n",
            "loss : 0.329676 [25600/60000]\n",
            "loss : 0.422036 [32000/60000]\n",
            "loss : 0.311036 [38400/60000]\n",
            "loss : 0.509722 [44800/60000]\n",
            "loss : 0.350981 [51200/60000]\n",
            "loss : 0.406617 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 86.8%, Avg loss: 0.36838942120788964\n",
            "\n",
            "loss : 0.364486 [    0/60000]\n",
            "loss : 0.634909 [ 6400/60000]\n",
            "loss : 0.347646 [12800/60000]\n",
            "loss : 0.401172 [19200/60000]\n",
            "loss : 0.231458 [25600/60000]\n",
            "loss : 0.510394 [32000/60000]\n",
            "loss : 0.420408 [38400/60000]\n",
            "loss : 0.233721 [44800/60000]\n",
            "loss : 0.211582 [51200/60000]\n",
            "loss : 0.345643 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 87.0%, Avg loss: 0.36480944674865456\n",
            "\n",
            "loss : 0.281489 [    0/60000]\n",
            "loss : 0.247633 [ 6400/60000]\n",
            "loss : 0.337160 [12800/60000]\n",
            "loss : 0.276951 [19200/60000]\n",
            "loss : 0.298538 [25600/60000]\n",
            "loss : 0.213936 [32000/60000]\n",
            "loss : 0.181979 [38400/60000]\n",
            "loss : 0.297223 [44800/60000]\n",
            "loss : 0.257717 [51200/60000]\n",
            "loss : 0.319571 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 87.2%, Avg loss: 0.36027297396568736\n",
            "\n",
            "loss : 0.220353 [    0/60000]\n",
            "loss : 0.337317 [ 6400/60000]\n",
            "loss : 0.289858 [12800/60000]\n",
            "loss : 0.306847 [19200/60000]\n",
            "loss : 0.299539 [25600/60000]\n",
            "loss : 0.342166 [32000/60000]\n",
            "loss : 0.359586 [38400/60000]\n",
            "loss : 0.301889 [44800/60000]\n",
            "loss : 0.235658 [51200/60000]\n",
            "loss : 0.234177 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 87.2%, Avg loss: 0.35851184103139644\n",
            "\n",
            "loss : 0.292384 [    0/60000]\n",
            "loss : 0.165925 [ 6400/60000]\n",
            "loss : 0.262251 [12800/60000]\n",
            "loss : 0.294645 [19200/60000]\n",
            "loss : 0.278497 [25600/60000]\n",
            "loss : 0.318583 [32000/60000]\n",
            "loss : 0.436478 [38400/60000]\n",
            "loss : 0.249058 [44800/60000]\n",
            "loss : 0.279092 [51200/60000]\n",
            "loss : 0.163335 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 87.3%, Avg loss: 0.3539861072400573\n",
            "\n",
            "loss : 0.431681 [    0/60000]\n",
            "loss : 0.293703 [ 6400/60000]\n",
            "loss : 0.262697 [12800/60000]\n",
            "loss : 0.301501 [19200/60000]\n",
            "loss : 0.275611 [25600/60000]\n",
            "loss : 0.233477 [32000/60000]\n",
            "loss : 0.172022 [38400/60000]\n",
            "loss : 0.228587 [44800/60000]\n",
            "loss : 0.226486 [51200/60000]\n",
            "loss : 0.527353 [57600/60000]\n",
            "Test Error:\n",
            " Accurancy: 87.4%, Avg loss: 0.3527792812722504\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9wMjY5aaCxu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "mnist.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}